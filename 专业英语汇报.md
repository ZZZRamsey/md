### VAE（2013.12）

- 发现问题：现有的 AE（Auto Encoder）学习的是一个离散的 latent feature ，记为 z ，

- 解决问题：

AE（自编码器） 系列的目的是为了学习一个瓶颈特征 z 的概率表示，也就是 z 的概率分布，从这个概率分布中进行抽样，拿这个抽样的特征 z，去做下游任务。

而 VAE 学习的不是 特征了，而是一个分布， 作者假设这个 分布是一个高斯分布（实际上并不是，只是用高斯分布去近似，因为高斯分布比较好求解），这个分布就可以用 (均值，方差) 来表示。

具体来说：当我们从编码器得到特征之后，我们在后面加 FC 层，用它去预测 均值 和 方差 ，之后从这个分布中采样一个 z 出来，这样 VAE 就可以做生成了，这个 z 就是一个可以从高斯分布中随机抽样出的一个样本。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/165cc84e385d2e60e2c0ce072fc74333.png)

从贝叶斯概率的角度来看， VAE 学的是一个概率分布，他从分布里去抽样，所以他生成图片的多样性 比 GAN 好很多

- 缺点： VAE 不好把图片的尺寸做大，而且 z 的分布也不是很好学。

- 参考：[VAE 到底在做什么？VAE 原理讲解系列#1_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1sM4y1W7jx?spm_id_from=333.788.recommend_more_video.-1&vd_source=3659941460e6c74679750d458eaa5726)



### VQ-VAE（2017.11）

[[1711.00937\] Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)

- 发现问题：

现实生活中，很多信息(声音、图片)都是连续的，你的大部分任务都是一个回归任务。但是等你真正将其表示出来或真正解决这些任务的时候，我们都将其离散化了。图像变成了像素，语音也抽样过了，大部分工作的很好的也都是分类模型 (回归任务转换成分类任务)。

如果还是之前 VAE 的模式，有以下两个缺点：	

第一：不好把图像尺寸做大，例如：当输入图像变大，潜在变量的分布可能会变得更加复杂，VAE 可能会面临计算复杂度高的问题。VQVAE 通过将潜在空间划分为离散的码本（codebook），可以在一定程度上降低计算复杂度。

第二：不好把模型做大，分布不好学，VAE 中的 Decoder 和 Encoder 分别表示 p(z|x)和 p(z|x），当 Decoder 和 Encoder 越来越复杂，高斯分布就越难以近似这两个分布。

- 解决问题：


结合了向量量化 （VQ） 的思想，不去直接预测分布，而是用一个 codebook 代替。codebook 可以理解为聚类的中心，大小一般是 K*D（K = 8192，Dim = 512/768），也就是有 8192 个长为 D 的向量。相当于又把连续的分布给离散化了，这样更好求解。同时也达到了去除了一些连续分布中没有意义的中间值的作用。

- 参考：[【双语】VQ-VAE ： Explanation and Implementation_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1BA4ReCEmf/?spm_id_from=333.337.search-card.all.click&vd_source=3659941460e6c74679750d458eaa5726)



### Diffusion（2015.03）

2015 年，斯坦福大学的一博士后 Sohl-Dickstein 通过此篇论文《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》提出扩散模型的概念。https://arxiv.org/abs/1503.03585
简单来讲，扩散模型的灵感来自非平衡热力学，通过定义了一个扩散步骤的马尔可夫链，以缓慢地将「符合高斯分布的随机噪声」添加到数据中，然后反转扩散过程以从噪声中构建所需的数据样本。



### DDPM（2020.06）

- 意义：如今生成扩散模型的大火，则是始于 2020 年所提出的 [DDPM 论文](https://papers.cool/arxiv/2006.11239)（Denoising Diffusion Probabilistic Model）

- 发现问题：以前的 diffusion 模型难以优化，计算复杂度过高。



- 解决问题：

  1. 既然直接预测噪声难以优化，那就只预测真实噪声和预测噪声之间的差值，有点 ResNet 的思想。简单来说，DDMP 的目标变成了学习残差。

  2. 引入了 time embedding（告诉模型现在去噪到了哪一步，是需要生成大致的图像轮廓，还是生成图像细节。简单的一个向量，起到了引导生成的作用）

  3. 预测噪声，就是预测一个正态分布（假设加的都是正态分布的噪声），预测分布就是预测均值和方差。DDPM 发现预测一个分布，甚至不用预测方差，将方差设置为一个常数，效果就已经很好了。



DDPM 的伪代码实现如下：

其中训练过程可以从任何的时间 t 处开始去噪，因为扩散模型是一个马尔科夫链，任何一个时刻的 t 的状态，都可以通过贝叶斯公式进行推算，最终表示为 t = 1 时刻的一个函数。

而推理过程，只能从 t = T 时刻（纯噪声）开始。

![image-20241023094612782](https://raw.githubusercontent.com/ZZZRamsey/PicGo_save/main/202411011423630.png)





### DALL-E（2021.02）

https://arxiv.org/abs/2102.12092

- 发现问题：以前的生成中，图片都是随机生成，缺乏可控性

- 解决问题：提出了用文本去指导图像生成（**引出了可控生成**，但并不是用的 diffusion，而是用的自回归模型来生成图像）

DALL-E 没有使用扩散模型，而是 dVAE（discrete variational autoencoder 离散变分自动编码器）。主要的卖点就是根据文本，去生成图像，并且用了自回归模型 GPT 来还原图像。

![img](https://i-blog.csdnimg.cn/blog_migrate/8c8b92037a4f235fe283db45a3ed1a02.jpeg)



整体流程如下：

1.第一个阶段，训练一个 dVAE（discrete variational autoencoder 离散变分自动编码器），其将 256 *256 的 RGB 图片转换为 32* 32 的图片 token。目的：降低图片的分辨率，从而解决计算量的问题。图片 token 的词汇量大小是 8192 个，即每个位置有 8192 种可能的取值 (也就是说 dVAE 的 encoder 输出是维度为 32x32x8192 的 logits，然后通过 logits 索引 codebook 的特征进行组合，codebook 的 embedding 是可学习的) 。第一阶段同时训练 dVAE 编码器和 dVAE 解码器。

2.第二阶段，用 BPE Encoder 对文本进行编码，得到最多 256 个文本 token，token 数不满 256 的话 padding 到 256，然后将 256 个文本 token 与 1024 个图像 token（32*32 = 1024）进行拼接，得到长度为 1280 的数据，用拼接的数据去训练一个自回归 transformer 来建模文本和图片 token 的联合分布。

文中主要和 GAN 相关模型进行比较，如 AttnGAN、DM-GAN、DF-GAM。



### Diffusion Model Beat GANs（2021.05）

论文地址：https://arxiv.org/abs/2105.05233

这是 Diffusion 模型首次在生成的保真度上超越 GAN 系列模型，提出了在 diffusion 中非常重要的技巧 **classifier guidance**，使用额外的一个模型，在每一步的噪声预测中指导噪声的生成，具体点来说，就是根据分类器所产生的梯度，判断扩散模型生成的效果，指导扩散模型生成。（这其中的分类器可以是许多其他的模型，比如说 CLIP，这样就可以用文本指导生成了 ）

这种技巧也 **扩散模型** 生成过程中 **开始变得可控**（而不是看运气生成）





### GLIDE（2021.12）

[GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/pdf/2112.10741)

OpenAI 看 Diffusion 已经 Beat GANs 了，所以放弃了 DALL-E 1 这种用 VQ-VAE 来做的路子，转而投向了 Diffusion。

- 发现问题：使用了 classifier guidance 的模型在训练时，总是会过于依赖分类器，无条件的生成效果没有有条件的好，可否让模型在无条件的情况下也达到有条件的效果？
- 解决问题：提出 c**lassifier-free guidance**

- 贡献：

探讨了文本条件图像合成问题的扩散模型，并比较了两种不同的指导策略： CLIP 指导和无分类器指导（ **classifier-free guidance**）。我们发现，后者是人类评估者在照片级真实感和字幕相似性方面的首选，并且经常产生照片级真实感样本。

 **classifier-free guidance** 被证明是一个很重要技巧，在训练过程中既生成有条件的指导的中间结果，也生成无条件指导的中间结果，然后对两个结果作差，这就告诉模型如何在无条件的情况下生成和有条件指导一样的效果。

在使用了很多技巧之后，基于扩散模型的 **GLIDE** 用了 3.5 亿参数，效果就直逼基于 VQ-VAE 的、用了 12 亿参数的 **DALL·E** 模型。OpenAI 看到扩散模型确实靠谱，所以顺着 GLIDE 的思路，孕育出了现在的 **DALL·E 2**



### LDM（2021.12）

[arxiv.org/pdf/2112.10752](https://arxiv.org/pdf/2112.10752)

- 发现问题：直接在图像层面上进行预测，计算量太大了。

- 解决问题：首次将将扩散过程带入到 latent space，在特征图层面进行加噪和去噪。让计算量大大降低，并且增加了生成模型的可控程度，引出了后来的 stabel diffusion。

参考：[Latent Diffusion：开始的开始 - 知乎](https://zhuanlan.zhihu.com/p/652186695)





### DALL-E 2（2022.04）

[2204.06125\] Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125)

DALL·E 2 建立在 GLIDE 的基础上，并通过使用 CLIP 图像嵌入而不是 GLIDE 中提出的原始文本嵌入来调节扩散过程，从而更进一步。将 GLIDE 改为层级式生成（56→256→1024）并加入 prior 网络等等。

参考：[DALL·E 2（内含扩散模型介绍）【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV17r4y1u77B?vd_source = 3659941460e6c74679750d458eaa5726)





## 参考博客：

[1] [图像生成发展起源：从 VAE、VQ-VAE、扩散模型 DDPM、DETR 到 ViT、Swin transformer-CSDN 博客](https://blog.csdn.net/v_JULY_v/article/details/130361959)

[2] [生成模型 | 从 VAE 到 Diffusion Model （上）_vae diffusion 时序-CSDN 博客](https://blog.csdn.net/qq_45842681/article/details/139163045?spm = 1001.2014.3001.5501)

[3] [生成模型 | 从 VAE 到 Diffusion Model （下）_vae diffusion-CSDN 博客](https://blog.csdn.net/qq_45842681/article/details/139167526)

[4] [李沐论文精读系列——由 DALL·E 2 看图像生成模型 - 知乎](https://zhuanlan.zhihu.com/p/593896912)

[5] [万字长文\]一篇文章带你理解Stable Diffusion是如何工作的 - 知乎](https://zhuanlan.zhihu.com/p/688914275)

[6] [文生图模型发展简史与原理：DALL·E, Imagen, Stable Diffusion - 知乎](https://zhuanlan.zhihu.com/p/667571081)